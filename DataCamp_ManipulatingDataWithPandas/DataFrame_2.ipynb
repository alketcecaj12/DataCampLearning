{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new columns\n",
    "You aren't stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\n",
    "\n",
    "You can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together, or by changing their units.\n",
    "\n",
    "homelessness is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region       state  individuals  family_members  state_pop\n",
       "0  East South Central     Alabama       2570.0           864.0    4887681\n",
       "1             Pacific      Alaska       1434.0           582.0     735139\n",
       "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
       "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
       "4             Pacific  California     109008.0         20964.0   39461588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "homelessness = pd.read_pickle('homeless_data.pkl')\n",
    "homelessness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region                 state  individuals  family_members  \\\n",
      "0   East South Central               Alabama       2570.0           864.0   \n",
      "1              Pacific                Alaska       1434.0           582.0   \n",
      "2             Mountain               Arizona       7259.0          2606.0   \n",
      "3   West South Central              Arkansas       2280.0           432.0   \n",
      "4              Pacific            California     109008.0         20964.0   \n",
      "5             Mountain              Colorado       7607.0          3250.0   \n",
      "6          New England           Connecticut       2280.0          1696.0   \n",
      "7       South Atlantic              Delaware        708.0           374.0   \n",
      "8       South Atlantic  District of Columbia       3770.0          3134.0   \n",
      "9       South Atlantic               Florida      21443.0          9587.0   \n",
      "10      South Atlantic               Georgia       6943.0          2556.0   \n",
      "11             Pacific                Hawaii       4131.0          2399.0   \n",
      "12            Mountain                 Idaho       1297.0           715.0   \n",
      "13  East North Central              Illinois       6752.0          3891.0   \n",
      "14  East North Central               Indiana       3776.0          1482.0   \n",
      "15  West North Central                  Iowa       1711.0          1038.0   \n",
      "16  West North Central                Kansas       1443.0           773.0   \n",
      "17  East South Central              Kentucky       2735.0           953.0   \n",
      "18  West South Central             Louisiana       2540.0           519.0   \n",
      "19         New England                 Maine       1450.0          1066.0   \n",
      "20      South Atlantic              Maryland       4914.0          2230.0   \n",
      "21         New England         Massachusetts       6811.0         13257.0   \n",
      "22  East North Central              Michigan       5209.0          3142.0   \n",
      "23  West North Central             Minnesota       3993.0          3250.0   \n",
      "24  East South Central           Mississippi       1024.0           328.0   \n",
      "25  West North Central              Missouri       3776.0          2107.0   \n",
      "26            Mountain               Montana        983.0           422.0   \n",
      "27  West North Central              Nebraska       1745.0           676.0   \n",
      "28            Mountain                Nevada       7058.0           486.0   \n",
      "29         New England         New Hampshire        835.0           615.0   \n",
      "30        Mid-Atlantic            New Jersey       6048.0          3350.0   \n",
      "31            Mountain            New Mexico       1949.0           602.0   \n",
      "32        Mid-Atlantic              New York      39827.0         52070.0   \n",
      "33      South Atlantic        North Carolina       6451.0          2817.0   \n",
      "34  West North Central          North Dakota        467.0            75.0   \n",
      "35  East North Central                  Ohio       6929.0          3320.0   \n",
      "36  West South Central              Oklahoma       2823.0          1048.0   \n",
      "37             Pacific                Oregon      11139.0          3337.0   \n",
      "38        Mid-Atlantic          Pennsylvania       8163.0          5349.0   \n",
      "39         New England          Rhode Island        747.0           354.0   \n",
      "40      South Atlantic        South Carolina       3082.0           851.0   \n",
      "41  West North Central          South Dakota        836.0           323.0   \n",
      "42  East South Central             Tennessee       6139.0          1744.0   \n",
      "43  West South Central                 Texas      19199.0          6111.0   \n",
      "44            Mountain                  Utah       1904.0           972.0   \n",
      "45         New England               Vermont        780.0           511.0   \n",
      "46      South Atlantic              Virginia       3928.0          2047.0   \n",
      "47             Pacific            Washington      16424.0          5880.0   \n",
      "48      South Atlantic         West Virginia       1021.0           222.0   \n",
      "49  East North Central             Wisconsin       2740.0          2167.0   \n",
      "50            Mountain               Wyoming        434.0           205.0   \n",
      "\n",
      "    state_pop     total  p_individuals  \n",
      "0     4887681    3434.0       0.748398  \n",
      "1      735139    2016.0       0.711310  \n",
      "2     7158024    9865.0       0.735834  \n",
      "3     3009733    2712.0       0.840708  \n",
      "4    39461588  129972.0       0.838704  \n",
      "5     5691287   10857.0       0.700654  \n",
      "6     3571520    3976.0       0.573441  \n",
      "7      965479    1082.0       0.654344  \n",
      "8      701547    6904.0       0.546060  \n",
      "9    21244317   31030.0       0.691041  \n",
      "10   10511131    9499.0       0.730919  \n",
      "11    1420593    6530.0       0.632619  \n",
      "12    1750536    2012.0       0.644632  \n",
      "13   12723071   10643.0       0.634408  \n",
      "14    6695497    5258.0       0.718144  \n",
      "15    3148618    2749.0       0.622408  \n",
      "16    2911359    2216.0       0.651173  \n",
      "17    4461153    3688.0       0.741594  \n",
      "18    4659690    3059.0       0.830337  \n",
      "19    1339057    2516.0       0.576312  \n",
      "20    6035802    7144.0       0.687850  \n",
      "21    6882635   20068.0       0.339396  \n",
      "22    9984072    8351.0       0.623758  \n",
      "23    5606249    7243.0       0.551291  \n",
      "24    2981020    1352.0       0.757396  \n",
      "25    6121623    5883.0       0.641849  \n",
      "26    1060665    1405.0       0.699644  \n",
      "27    1925614    2421.0       0.720777  \n",
      "28    3027341    7544.0       0.935578  \n",
      "29    1353465    1450.0       0.575862  \n",
      "30    8886025    9398.0       0.643541  \n",
      "31    2092741    2551.0       0.764014  \n",
      "32   19530351   91897.0       0.433387  \n",
      "33   10381615    9268.0       0.696051  \n",
      "34     758080     542.0       0.861624  \n",
      "35   11676341   10249.0       0.676066  \n",
      "36    3940235    3871.0       0.729269  \n",
      "37    4181886   14476.0       0.769481  \n",
      "38   12800922   13512.0       0.604130  \n",
      "39    1058287    1101.0       0.678474  \n",
      "40    5084156    3933.0       0.783626  \n",
      "41     878698    1159.0       0.721311  \n",
      "42    6771631    7883.0       0.778764  \n",
      "43   28628666   25310.0       0.758554  \n",
      "44    3153550    2876.0       0.662031  \n",
      "45     624358    1291.0       0.604183  \n",
      "46    8501286    5975.0       0.657406  \n",
      "47    7523869   22304.0       0.736370  \n",
      "48    1804291    1243.0       0.821400  \n",
      "49    5807406    4907.0       0.558386  \n",
      "50     577601     639.0       0.679186  \n"
     ]
    }
   ],
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_individuals col as proportion of individuals\n",
    "homelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combo-attack!\n",
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\n",
    "\n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state  indiv_per_10k\n",
      "8   District of Columbia      53.738381\n",
      "11                Hawaii      29.079406\n",
      "4             California      27.623825\n",
      "37                Oregon      26.636307\n",
      "28                Nevada      23.314189\n",
      "47            Washington      21.829195\n",
      "32              New York      20.392363\n"
     ]
    }
   ],
   "source": [
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and median\n",
    "Summary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there's a lot of it.\n",
    "\n",
    "sales is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413119, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>type</th>\n",
       "      <th>department</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>50605.27</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>13740.12</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>39954.04</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>32229.38</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store type  department       date  weekly_sales  is_holiday  temperature_c  \\\n",
       "0      1    A           1 2010-02-05      24924.50       False       5.727778   \n",
       "1      1    A           2 2010-02-05      50605.27       False       5.727778   \n",
       "2      1    A           3 2010-02-05      13740.12       False       5.727778   \n",
       "3      1    A           4 2010-02-05      39954.04       False       5.727778   \n",
       "4      1    A           5 2010-02-05      32229.38       False       5.727778   \n",
       "\n",
       "   fuel_price_usd_per_l  unemployment  \n",
       "0              0.679451         8.106  \n",
       "1              0.679451         8.106  \n",
       "2              0.679451         8.106  \n",
       "3              0.679451         8.106  \n",
       "4              0.679451         8.106  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_pickle('walmart_sales.pkl')\n",
    "print(sales.shape)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store type  department       date  weekly_sales  is_holiday  temperature_c  \\\n",
      "0      1    A           1 2010-02-05      24924.50       False       5.727778   \n",
      "1      1    A           2 2010-02-05      50605.27       False       5.727778   \n",
      "2      1    A           3 2010-02-05      13740.12       False       5.727778   \n",
      "3      1    A           4 2010-02-05      39954.04       False       5.727778   \n",
      "4      1    A           5 2010-02-05      32229.38       False       5.727778   \n",
      "\n",
      "   fuel_price_usd_per_l  unemployment  \n",
      "0              0.679451         8.106  \n",
      "1              0.679451         8.106  \n",
      "2              0.679451         8.106  \n",
      "3              0.679451         8.106  \n",
      "4              0.679451         8.106  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 413119 entries, 0 to 413118\n",
      "Data columns (total 9 columns):\n",
      "store                   413119 non-null int64\n",
      "type                    413119 non-null object\n",
      "department              413119 non-null int32\n",
      "date                    413119 non-null datetime64[ns]\n",
      "weekly_sales            413119 non-null float64\n",
      "is_holiday              413119 non-null bool\n",
      "temperature_c           413119 non-null float64\n",
      "fuel_price_usd_per_l    413119 non-null float64\n",
      "unemployment            413119 non-null float64\n",
      "dtypes: bool(1), datetime64[ns](1), float64(4), int32(1), int64(1), object(1)\n",
      "memory usage: 27.2+ MB\n",
      "None\n",
      "16094.726811185497\n",
      "7682.47\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing dates\n",
    "Summary statistics can also be calculated on date columns which have values with the data type datetime64. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example minimum and maximum, which allow you to see what time range your data covers.\n",
    "\n",
    "sales is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-10-26 00:00:00\n",
      "2010-02-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient summaries\n",
    "While pandas and NumPy have tons of functions, sometimes you may need a different function to summarize your data.\n",
    "\n",
    "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super efficient.\n",
    "\n",
    "In the custom function for this exercise, \"IQR\" is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. It's an alternative to standard deviation that is helpful if your data contains outliers.\n",
    "\n",
    "sales is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.299999999999994\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           15.300000\n",
      "fuel_price_usd_per_l     0.211866\n",
      "unemployment             1.672000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr             15.30              0.211866         1.672\n",
      "median          16.75              0.911922         7.852\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicates\n",
    "Removing duplicates is an essential skill to get accurate counts, because often you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from sales.\n",
    "\n",
    "sales is available and pandas is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       store type  department       date  weekly_sales  is_holiday  \\\n",
      "0          1    A           1 2010-02-05      24924.50       False   \n",
      "10244      2    A           1 2010-02-05      35034.06       False   \n",
      "20482      3    B           1 2010-02-05       6453.58       False   \n",
      "29518      4    A           1 2010-02-05      38724.42       False   \n",
      "39790      5    B           1 2010-02-05       9323.89       False   \n",
      "\n",
      "       temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0           5.727778              0.679451         8.106  \n",
      "10244       4.550000              0.679451         8.324  \n",
      "20482       7.616667              0.679451         7.368  \n",
      "29518       6.533333              0.686319         8.623  \n",
      "39790       4.277778              0.679451         6.566  \n",
      "   store type  department       date  weekly_sales  is_holiday  temperature_c  \\\n",
      "0      1    A           1 2010-02-05      24924.50       False       5.727778   \n",
      "1      1    A           2 2010-02-05      50605.27       False       5.727778   \n",
      "2      1    A           3 2010-02-05      13740.12       False       5.727778   \n",
      "3      1    A           4 2010-02-05      39954.04       False       5.727778   \n",
      "4      1    A           5 2010-02-05      32229.38       False       5.727778   \n",
      "\n",
      "   fuel_price_usd_per_l  unemployment  \n",
      "0              0.679451         8.106  \n",
      "1              0.679451         8.106  \n",
      "2              0.679451         8.106  \n",
      "3              0.679451         8.106  \n",
      "4              0.679451         8.106  \n",
      "73     2010-02-12\n",
      "2218   2010-09-10\n",
      "3014   2010-11-26\n",
      "3372   2010-12-31\n",
      "3800   2011-02-11\n",
      "5940   2011-09-09\n",
      "6731   2011-11-25\n",
      "7096   2011-12-30\n",
      "7527   2012-02-10\n",
      "9667   2012-09-07\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows that are holiday weeks and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting categorical variables\n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "The store_types and store_depts DataFrames you created in the last exercise are available and pandas is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    22\n",
      "B    17\n",
      "C     6\n",
      "Name: type, dtype: int64\n",
      "A    0.488889\n",
      "B    0.377778\n",
      "C    0.133333\n",
      "Name: type, dtype: float64\n",
      "1     45\n",
      "9     45\n",
      "4     45\n",
      "6     45\n",
      "8     45\n",
      "      ..\n",
      "37    20\n",
      "50    14\n",
      "43     5\n",
      "39     5\n",
      "65     1\n",
      "Name: department, Length: 81, dtype: int64\n",
      "1     0.013778\n",
      "9     0.013778\n",
      "4     0.013778\n",
      "6     0.013778\n",
      "8     0.013778\n",
      "        ...   \n",
      "37    0.006124\n",
      "50    0.004287\n",
      "43    0.001531\n",
      "39    0.001531\n",
      "65    0.000306\n",
      "Name: department, Length: 81, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
